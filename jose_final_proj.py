# -*- coding: utf-8 -*-
"""Jose final proj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IKqn5KKtiliWaVByuUSpR_1xlJPHbNBE
"""

# for data reading and data manipulation
import numpy as np
import pandas as pd
import statistics as st

# for data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# for model creation and model evaluation
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

from google.colab import files
uploaded = files.upload()
import os
print(os.listdir())  # List all files in the current directory

# reading data from a .csv file to a Pandas DataFrame
df = pd.read_csv('train.csv')
pd.set_option('display.max_columns',None)
pd.set_option('display.max_rows',None)
df.head()

#checking no. of columns & rows
df.shape

#checking the column titles
df.columns

"""Checking for missing values"""

#looking for the amount of null data in the pandas dataframe
df.isnull().sum()

"""Data type correction"""

df.dtypes

"""The categorical columns are having too many categories, seems like it is needed to analyze all of them one by one."""

# for column "age"
df['age'].value_counts()

# for column "job"
df['job'].value_counts()

# for column "marital"
df['marital'].value_counts()

# for column "education_qual"
df['education_qual'].value_counts()

# for column "call_type"
df['call_type'].value_counts()

# for column "	day"
df['day'].value_counts()

# for column "mon"
df['mon'].value_counts()

# for column "dur"
df['dur'].value_counts()

# for column "num_calls"
df['num_calls'].value_counts()

# for column "prev_outcome"
df['prev_outcome'].value_counts()

# for column "y"
df['y'].value_counts()

#Remove duplicates
df = df.drop_duplicates()

df.shape

"""Data Visualization"""

!pip install seaborn==0.11

#plotting count plots for all the categorical columns
sns.set_theme(style='darkgrid',palette='pastel')

plt.figure(figsize=(20,25))
plt.subplot(431)
sns.countplot(df['age'],order=df['age'].value_counts().index[::-1])
plt.xlabel('Age')
plt.ylabel('Count')
plt.title('Customer conversion according to Age')

plt.subplot(432)
sns.countplot(df['job'],order=df['job'].value_counts().index[::-1])
plt.xlabel('job')
plt.ylabel('Count')
plt.title('Customer conversion according to job')

plt.subplot(433)
sns.countplot(df['marital'],order=df['marital'].value_counts().index[::-1])
plt.xlabel('marital')
plt.ylabel('Count')
plt.title('Customer conversion according to marital')

plt.subplot(434)
sns.countplot(df['education_qual'],order=df['education_qual'].value_counts().index[::-1])
plt.xlabel('education_qual')
plt.ylabel('Count')
plt.title('Customer conversion according to education_qual')

plt.subplot(435)
sns.countplot(df['call_type'],order=df['call_type'].value_counts().index[::-1])
plt.xlabel('call_type')
plt.ylabel('Count')
plt.title('Customer conversion according to call_type')

plt.subplot(436)
sns.countplot(df['day'],order=df['day'].value_counts().index[::-1])
plt.xlabel('day')
plt.ylabel('Count')
plt.title('Customer conversion according to day')

plt.subplot(437)
sns.countplot(df['mon'],order=df['mon'].value_counts().index[::-1])
plt.xlabel('mon')
plt.ylabel('Count')
plt.title('Customer conversion according to mon')

plt.subplot(438)
sns.countplot(df['dur'],order=df['dur'].value_counts().index[::-1])
plt.xlabel('dur')
plt.ylabel('Count')
plt.title('Customer conversion according to dur')

plt.subplot(439)
sns.countplot(df['num_calls'],order=df['num_calls'].value_counts().index[::-1])
plt.xlabel('num_calls')
plt.ylabel('Count')
plt.title('Customer conversion according to num_calls')

plt.subplot(4,3,10)
sns.countplot(df['prev_outcome'],order=df['prev_outcome'].value_counts().index[::-1])
plt.xlabel('prev_outcome')
plt.ylabel('Count')
plt.title('Customer conversion according to prev_outcome')

plt.subplot(4,3,11)
sns.countplot(df['y'],order=df['y'].value_counts().index[::-1])
plt.xlabel('y')
plt.ylabel('Count')
plt.title('Customer conversion according to y')

plt.tight_layout()

#Plotting barplots for Categorical Columns vs Mean y
#Plotting barplots between  y vs age
plt.figure(figsize=(20,25))
plt.subplot(431)
sns.barplot(df['y'],df['age'], order = df.groupby('y')['age'].mean().reset_index().sort_values('age')['y'])
plt.xlabel('y')
plt.ylabel('age')
plt.title('Age vs y')

"""Encoding Categorical Variables"""

df['job'] = df['job'].map({'blue-collar':0,'management':1,'technician':2,'admin.':3,'services':4,'retired':5,'self-employed':6, 'entrepreneur':7, 'unemployed':8, 'housemaid':9, 'student':10, 'unknown':11})
df['marital'] = df['marital'].map({'married':0,'single':1,'divorced':2})
df['education_qual'] = df['education_qual'].map({'secondary':0,'tertiary':1,'primary':2,'unknown':4})
df['call_type'] = df['call_type'].map({'cellular':0,'unknown':1, 'telephone':2})
df['mon'] = df['mon'].map({'may':0,'jul':1, 'aug':2, 'jun':3,'nov':4,'apr':5, 'feb':6, 'jan':7, 'oct':8, 'sep':9, 'mar':10, 'dec ':11})
df['prev_outcome'] = df['prev_outcome'].map({'unknown':0,'failure':1, 'other':2, 'success':3})
df['y'] = df['y'].map({'no':0,'yes':1})

df.head()

"""Splitting the data into input data and output data"""

X = df.drop('y',axis=1)
y = df['y']

X

y

#splitting the data into training and testing sets with the ratio of 8:2
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=70)

print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)

X_train

"""**Building Machine Learning Model**"""

from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

"""I. Linear regression"""

from sklearn.impute import SimpleImputer

# Assuming X_train and X_test have missing values
imputer = SimpleImputer(strategy='mean')

# Fit the imputer on the training data and transform both training and testing data
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Now use the imputed data to fit the linear model
linear_model.fit(X_train_imputed, y_train)
predictions = linear_model.predict(X_test_imputed)

from sklearn.linear_model import LinearRegression #import
linear_model = LinearRegression(fit_intercept=True) #initialise
linear_model.fit(X_train_imputed,y_train) #fit - all magic
print(linear_model.predict(X_test_imputed))     #predict
print(y_test)

linear_model.score(X_test_imputed, y_test)

from sklearn.model_selection import cross_val_score
# synatx : cross_val_score(model, fts_train, target_train, bins).mean()
cross_val_linear_model=cross_val_score(linear_model,X_train_imputed,y_train,cv=10).mean()
cross_val_linear_model

"""II. K Nearest Neighbor Regression

II.a.Choosing the best K(neighbor) Value
"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train_imputed)
X_train_imputed_scaled = scaler.transform(X_train_imputed)
X_test_imputed_scaled = scaler.transform(X_test_imputed)

knn_values=np.arange(1,50)
cross_val_knn=[]
for k in knn_values:
  knn_regressor=KNeighborsRegressor(n_neighbors=k)
  knn_regressor.fit(X_train_imputed_scaled,y_train)
  print("K value : ", k, " train score : ", knn_regressor.score(X_train_imputed_scaled,y_train)  ,"cross_val_score : ", cross_val_score(knn_regressor,X_train_imputed_scaled,y_train,cv = 10).mean())
  cross_val_knn.append(cross_val_score(knn_regressor,X_train_imputed_scaled,y_train,cv = 10).mean())

cross_val_knn_regressor=max(cross_val_knn)

print("The best K-Value is 3 and Cross_val_score is",cross_val_knn_regressor )

""" II.b.Implementing K Nearest Neighbor Regression"""

knn_regressor=KNeighborsRegressor(n_neighbors=3)
knn_regressor.fit(X_train_imputed_scaled,y_train)

cross_val_knn_regressor=cross_val_score(knn_regressor,X_train_imputed_scaled,y_train,cv=15).mean()
cross_val_knn_regressor

"""III.Decision Tree Regression

III.a.Choosing the best of depth Value
"""

from sklearn.tree import DecisionTreeRegressor

max_depth=np.arange(1,20)
cross_val_dt=[]
for d in max_depth:
  dt_regressor= DecisionTreeRegressor(max_depth=d, random_state=0)
  dt_regressor.fit(X_train_imputed,y_train)
  print("Depth : ", d, " train Score  : ", dt_regressor.score(X_train_imputed,y_train), "cross_val_score : ", cross_val_score(dt_regressor,X_train_imputed,y_train,cv = 10).mean())
  cross_val_dt.append(cross_val_score(dt_regressor,X_train_imputed,y_train,cv = 10).mean())

cross_val_dt_regressor=max(cross_val_dt)

print("The best depth is 14 and Cross_val_score is:",cross_val_dt_regressor)

"""III.b.Implementing Decision Tree Regression"""

dt_regressor=DecisionTreeRegressor(max_depth=14, random_state=0)
dt_regressor.fit(X_train_imputed,y_train)

cross_val_dt_regressor=cross_val_score(dt_regressor,X_train_imputed,y_train,cv=10).mean()
cross_val_dt_regressor

ftImp = list(zip(dt_regressor.feature_importances_, df.columns[:-1]))
imp = pd.DataFrame(ftImp, columns = ["Importance","Feature"])
imp.sort_values("Importance",ascending = False,inplace=True)
imp

"""IV.Random Forest Regression

IV.a.Choosing the best depth value
"""

from sklearn.ensemble import RandomForestRegressor

max_depth=np.array([2,4,8,10,11,12,13,15,18,20])
cross_val_rf=[]
for d in max_depth:
  rf_regressor=RandomForestRegressor(max_depth=d, random_state=0)
  rf_regressor.fit(X_train_imputed,y_train)
  print("Depth : ", d, "cross_val_score : ", cross_val_score(rf_regressor,X_train_imputed,y_train,cv = 15).mean())
  cross_val_rf.append(cross_val_score(rf_regressor,X_train_imputed,y_train,cv = 15).mean())

cross_val_rf_regressor=max(cross_val_rf)

print("The best depth is 20 and Cross_val_score is:",cross_val_rf_regressor)

"""IV.b.Implementing Random Forest Regression"""

rf_regressor=RandomForestRegressor(max_depth=20, random_state=0)
rf_regressor.fit(X_train_imputed,y_train)

cross_val_rf_regressor=cross_val_score(rf_regressor,X_train_imputed,y_train,cv=15).mean()
cross_val_rf_regressor

"""V.Extreme Gradient Boosting Regression

V.a.Choosing the best Learning Rate
"""

import xgboost as xgb

cross_val_xgb=[]
for lr in [0.01,0.05,0.08,0.1,0.2,0.25,0.3]:
  xgb_regressor= xgb.XGBRegressor(learning_rate = lr,n_estimators=100)
  xgb_regressor.fit(X_train_imputed,y_train)
  print("Learning rate : ", lr,"cross_val_score:", cross_val_score(xgb_regressor,X_train_imputed,y_train,cv = 15).mean())
  cross_val_xgb.append(cross_val_score(xgb_regressor,X_train_imputed,y_train,cv = 15).mean())

cross_val_xgb_regressor=max(cross_val_xgb)

print("The best Learning rate is 0.1 and Cross_val_score is:",cross_val_xgb_regressor)

"""V.b.Implementing Extreme Gradient Boosting Regression"""

xgb_regressor= xgb.XGBRegressor(learning_rate =0.1,n_estimators=100) # initialise the model
  xgb_regressor.fit(X_train_imputed,y_train) #train the model

cross_val_xgb_regressor=cross_val_score(xgb_regressor,X_train_imputed,y_train,cv=15).mean()
cross_val_xgb_regressor

"""VI.Cross-Validation Score for Machine-Learning Models"""

print("Cross Validation Score for Linear Regression Model:",cross_val_linear_model)
print("Cross Validation Score for K-Nearest Neighbors Regression Model:",cross_val_knn_regressor)
print("Cross Validation Score for Decision Tree Regression Model: ",cross_val_dt_regressor)
print("Cross Validation Score for Random Forest Regression Model: ",cross_val_rf_regressor)
print("Cross Validation Score for Extreme-Gradient Boosting Regression Model: ",cross_val_xgb_regressor)

"""VII.R2 Score for Machine-Learning Models"""

from sklearn.metrics import r2_score

y_pred_lr=linear_model.predict(X_test_imputed)
y_pred_knn=knn_regressor.predict(X_test_imputed)
y_pred_dt= dt_regressor.predict(X_test_imputed)
y_pred_rf=rf_regressor.predict(X_test_imputed)
y_pred_xgb=xgb_regressor.predict(X_test_imputed)

R2_score_lr=r2_score(y_test,y_pred_lr)
R2_score_knn=r2_score(y_test,y_pred_knn)
R2_score_dt=r2_score(y_test,y_pred_dt)
R2_score_rf=r2_score(y_test,y_pred_rf)
R2_score_xgb=r2_score(y_test,y_pred_xgb)

print("R2 Score for Linear Regression Model:",R2_score_lr)
print("R2 Score for K-Nearest Neighbors Regression Model:",R2_score_knn)
print("R2 Score for Decision Tree Regression Model: ",R2_score_dt)
print("R2 Score for Random Forest Regression Model: ",R2_score_rf)
print("R2 Score for Extreme-Gradient Boosting Regression Model: ",R2_score_xgb)

"""Suggestion to Sellers and buyers-Solving problem statements based on Feature Importance"""

xgb_regressor.feature_importances_

df.columns

sorted_idx = xgb_regressor.feature_importances_.argsort()
plt.figure(figsize=(10,5))
plt.barh(df.columns[sorted_idx], xgb_regressor.feature_importances_[sorted_idx])
plt.xlabel("Random Forest Feature Importance")
plt.title("Feature Importance")
plt.show()

xgb_regressor.feature_importances_

sorted_idx = xgb_regressor.feature_importances_.argsort()
plt.figure(figsize=(10,5))
plt.barh(df.columns[sorted_idx], xgb_regressor.feature_importances_[sorted_idx])
plt.xlabel("Extreme Gradient Boosting Feature Importance")
plt.title("Feature Importance")
plt.show()

